---
layout: post
title: Project cantaloop - Local LLM setup
hidden: true
date: 2025-12-08 08:00:00 +0200
categories: [linux,software,ai,llm]
tags: [linux,datacenter]
---

# ðŸŒ Project cantaloop - Local LLM setup

*Documenting steps setting up my server.*

### Steps

> âœ… ollama<br>
> âœ… lmstudio<br>



### Local LLM

```bash
sudo apt update && sudo apt upgrade -y

```


### Open Source Models

CPU oriented<br>
Llama<br>
Gemma 3<br>
gpt-oss<br>

>
```bash
sudo docker stop open-webui 
 
sudo docker rm open-webui
sudo docker pull ghcr.io/open-webui/open-webui:0.5.0

sudo docker run -d \
  --name open-webui \
  -p 3000:8080 \
  -v /opt/open-webui/data:/app/backend/data \
  -v /opt/open-webui/logs:/app/backend/logs \
  -e WEBUI_URL_PREFIX=/ui \
  --restart unless-stopped \
  ghcr.io/open-webui/open-webui:0.4.6

sudo docker ps

sudo docker pull ghcr.io/open-webui/open-webui:0.4.6

sudo docker exec -it open-webui ls /app/build/static

curl -I https://llm.cantaloop.dk/ui/static/loader.js


------------
sudo docker stop open-webui
sudo docker rm open-webui
sudo docker pull ghcr.io/open-webui/webui:latest

location /ui/ {
    rewrite ^/ui/(.*)$ /$1 break;

    proxy_pass http://127.0.0.1:3000/;

    proxy_http_version 1.1;
    proxy_set_header Connection "";
    proxy_set_header Host $host;
    proxy_set_header X-Real-IP $remote_addr;
    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    proxy_set_header X-Forwarded-Proto $scheme;

    proxy_read_timeout 3600s;
    proxy_send_timeout 3600s;
}


sudo nginx -t
sudo systemctl reload nginx

curl -I https://llm.cantaloop.dk/ui/_app/immutable/entry/start.aabf9670.js

sudo docker pull ghcr.io/open-webui/open-webui:0.5.12
sudo docker run -d \
  --name open-webui \
  -p 3000:8080 \
  -v /opt/open-webui/data:/app/backend/data \
  -v /opt/open-webui/logs:/app/backend/logs \
  -e WEBUI_URL_PREFIX=/ui \
  --restart unless-stopped \
  ghcr.io/open-webui/open-webui:0.5.12



dropped docker based

now git, python, clone into /opt/open-webui-src (with tom:tom peromission)

stop using /ui for redirect - then all works at llm.cantaloop.dk
last minute problems with docker access, had to adjust ufw

memory - too little for models
free -h

ahh api/chat stuff nu ny version


```



### ðŸ’¡Still to do

> ðŸ”­ just getting started<br>
<br>
---